{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Model for Classification Toxic Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook content:\n",
    "\n",
    "1. Import Libraries\n",
    "2. Run Bert Model on TPU\n",
    "3. Functions and Variables<br>\n",
    "    3.1 Function for Encoding the comment<br>\n",
    "    3.2 Function for Neural Network model<br>\n",
    "4. Preprocessing\n",
    "    4.1 Import Datasets<br>\n",
    "    4.2 tokenaizer <br>\n",
    "    4.3 Encode The Comments<br>\n",
    "    4.4 Prepare tensorflow dataset for modeling<br>\n",
    "5. Machine Learning<br>\n",
    "    5.1 Training The Model, Tuning Hyper-Parameters<br>\n",
    "    5.2 Testing The Model\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(40)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer, Tokenizer, models, pre_tokenizers, decoders, processors\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Run Bert Model on TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Functions and Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Function for Encoding the comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "        # encode the word to vector of integer\n",
    "\n",
    "    encode_dictionary = tokenizer.batch_encode_plus(texts, return_attention_masks=False, return_token_type_ids=False,\n",
    "    pad_to_max_length=True,max_length=maxlen)\n",
    "    \n",
    "    return np.array(encode_dictionary['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Function for Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "\n",
    "#Input: for define input layer\n",
    "#shape is vector with 512-dimensional vectors\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "# to get the vector\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "# define output layer\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "# initiate the model with inputs and outputs\n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "# get the learning rate adam(1e-5) and the metrica\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define variables for modeling use\n",
    "EPOCHS = 3 #number of epochs in model\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync # the batch size in each epoch (128)\n",
    "MAX_LEN = 192\n",
    "\n",
    "# distilbert pre-trained model is faster than the bert base model, but it give lower accuracy than the bert base\n",
    "#MODEL ='distilbert-base-multilingual-cased'\n",
    "\n",
    "MODEL='bert-base-multilingual-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API to build highly flexible and efficient TensorFlow input pipelines.\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Import Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(\"../data/jigsaw-toxic-comment-train.csv\")\n",
    "\n",
    "valid = pd.read_csv('../data/validation.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "sub = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 tokenaizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the pre-trained model bert as a tokenizer \n",
    "#bert tokenizer has vocabulary for emoji. this is the reason we don't need to remove emoji from \n",
    "#datasets, for more details see the (EDA & data cleaning) notebook\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Encode The Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 22s, sys: 442 ms, total: 14min 22s\n",
      "Wall time: 14min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "#call the function regular encode on for all the 3 dataset to convert each words after the tokenizer\n",
    "#into a vector\n",
    "#x_train,x_test, and x_validation will have the comment text column only,(in test called \"content\")\n",
    "x_train = regular_encode(train1.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "#y_train,y_valid will have te target column \"toxic\"\n",
    "y_train = train1.toxic.values\n",
    "y_valid = valid.toxic.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Prepare Tensorflow Dataset For Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and prepare a source dataset from your input data to fit the model in the next step.\n",
    "# Apply dataset transformations to preprocess the data.\n",
    "# Iterate over the dataset and process the elements.\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset # create dataset\n",
    "    .from_tensor_slices((x_train, y_train)) # Once you have a dataset, you can apply transformations \n",
    "    .repeat()\n",
    "    .shuffle(2048,seed=40) # Combines consecutive elements of this dataset into batches.\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)) #This allows later elements to be prepared while the current element is being processed (pipline).\n",
    "\n",
    "\n",
    "valid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(BATCH_SIZE)\n",
    "    .cache().prefetch(AUTO))\n",
    "\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(x_test).batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model_2 (TFBertModel ((None, 192, 768), (None, 177853440 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice_2  [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 769       \n",
      "=================================================================\n",
      "Total params: 177,854,209\n",
      "Trainable params: 177,854,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 13.4 s, sys: 9.68 s, total: 23.1 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# in the TPU\n",
    "with strategy.scope():\n",
    "    #take the encoder results of bert from transformers and use it as an input in the NN model\n",
    "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Training The Model, Tuning Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1746 steps, validate for 63 steps\n",
      "Epoch 1/3\n",
      "1746/1746 [==============================] - 439s 252ms/step - loss: 0.1160 - auc_1: 0.9696 - val_loss: 0.4980 - val_auc_1: 0.8348\n",
      "Epoch 2/3\n",
      "1746/1746 [==============================] - 336s 192ms/step - loss: 0.0880 - auc_1: 0.9831 - val_loss: 0.5192 - val_auc_1: 0.8106\n",
      "Epoch 3/3\n",
      "1746/1746 [==============================] - 335s 192ms/step - loss: 0.0744 - auc_1: 0.9884 - val_loss: 0.5028 - val_auc_1: 0.8091\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "# training the data and tune our model with the results of the metrics we get from the validation dataset\n",
    "n_steps = x_train.shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(train_dataset, steps_per_epoch=n_steps, validation_data=valid_dataset,\n",
    "                epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Testing The Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 62 steps\n",
      "Epoch 1/6\n",
      "62/62 [==============================] - 57s 913ms/step - loss: 0.2973 - auc_1: 0.8689\n",
      "Epoch 2/6\n",
      "62/62 [==============================] - 51s 821ms/step - loss: 0.2074 - auc_1: 0.9434\n",
      "Epoch 3/6\n",
      "62/62 [==============================] - 12s 191ms/step - loss: 0.1375 - auc_1: 0.9768\n",
      "Epoch 4/6\n",
      "62/62 [==============================] - 12s 189ms/step - loss: 0.0925 - auc_1: 0.9883\n",
      "Epoch 5/6\n",
      "62/62 [==============================] - 12s 191ms/step - loss: 0.0693 - auc_1: 0.9937\n",
      "Epoch 6/6\n",
      "62/62 [==============================] - 12s 190ms/step - loss: 0.0505 - auc_1: 0.9968\n"
     ]
    }
   ],
   "source": [
    "#test the model on validation\n",
    "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
    "train_history_2 = model.fit(valid_dataset.repeat(), steps_per_epoch=n_steps,epochs=EPOCHS*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499/499 [==============================] - 59s 119ms/step\n"
     ]
    }
   ],
   "source": [
    "#predict and submit\n",
    "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.167953e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.725290e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.748423e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.104044e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9.536743e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id         toxic\n",
       "0   0  1.167953e-03\n",
       "1   1  3.725290e-06\n",
       "2   2  9.748423e-01\n",
       "3   3  2.104044e-05\n",
       "4   4  9.536743e-07"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
