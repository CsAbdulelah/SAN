{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing(NLP) is one of the main usages for the Neural networks-Deep learning model wither it was speech recognition, ChatBots or even predict the next words in a sentence, this all will not be achieved throughout using simple NN there is model's developed in order to overcome these obstacles one of these models is RNN.\n",
    "- RNN - (Recurrent Neural Network) is a generalization of a feedforward neural network that has internal memory. It's recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation.\n",
    "\n",
    "<img src=\"../assets/RNN.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "One of the RNN limitations is Gradient vanishing and exploding problems, That's why the model  LSTM was founded.\n",
    "- LSTM -(Long Short Term Memory)It's a special kind of RNN, capable of learning long-term dependencies.This was created with one basic thing in mind- the gradients shouldnâ€™t vanish even if the sequence is very large.\n",
    "\n",
    "<img src=\"../assets/LSTM.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "To Improve the model performance on sequence classification problems Bidirectional LSTMs have used.\n",
    "- BiLSTM - (Bidirectional LSTMs) it's an extension of traditional LSTMs. It trains two instead of one LSTMs on the input sequence, The first on the input sequence as-is, and the second on a reversed copy of the input sequence. This can provide additional context to the network and result in faster and even fuller learning on the problem.\n",
    "\n",
    "<img src=\"../assets/BI-LSTM.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "In our case study for the detection of Toxic comments, it's very important to consider the context and sequence of a word that's why we used the BiLSTM model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 . Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook created for the Explanation & submission purposes, for  use the link provided to run the code on Kaggle Notebook using GPU: \n",
    "https://www.kaggle.com/norahsh/2-lstm-model-for-toxic-classification-comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Text data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed the model we first need to vectorize the comments, This can be done by using keras ```Tokenizer``` it will break down the sentence --> put the words in a dictionary-like structure and give an index for each --> represent the sequence of words in the comments in the form of index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features,lower=True,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "#Fitting tokenizer\n",
    "tokenizer.fit_on_texts(list(list_sentences_train) + list(list_sentences_validation) + list(list_sentences_test))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Building training set\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list(list_sentences_train))\n",
    "y_train = train['toxic'].values\n",
    "\n",
    "# Building validation set\n",
    "list_tokenized_validation = tokenizer.texts_to_sequences(list(list_sentences_validation))\n",
    "y_valid = valid['toxic'].values\n",
    "\n",
    "# Building test set\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list(list_sentences_test))\n",
    "\n",
    "del tokenizer # To save RAM space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use `pad_sequences` in order to fix the issue of inconsistent length we use the max length feature to fill the short sentences with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200 # length of padding\n",
    "\n",
    "# Padding sequences for all \n",
    "print('Padding sequences...')\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_valid = pad_sequences(list_tokenized_validation, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In thes step We use Pre-trained word embeddings wich are usually trained on a large amount of corpus it helps avoid training word embedding from scratch in here i comined 2 Pre-trained word embeddings Glove & crawl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE:</b> The training dataset has only English comments to handle the language problem. We combined two-word vectors embedding (FastText(Crawl) & Glove) it's Pre-Trined word vectors depend on the word representation in the vector space and each of them has her own way.\n",
    "The second thing we did is to fit the validation data set after the train, So all this helps the model to deal with different languages and give accuracy equal to 78%.\n",
    "There is a lot of technique that can be implemented in the future like injection the embedding with different words and using cross-lingual word embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 load pre trined word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Fasttext word vector\n",
    "with open('../input/pickled-crawl300d2m-for-kernel-competitions/crawl-300d-2M.pkl', 'rb') as  infile:\n",
    "        crawl_embeddings = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GloVe word vector\n",
    "with open('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', 'rb') as  infile:\n",
    "        glove_embeddings = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Build embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(word_index, embeddings_index):\n",
    "    \n",
    "    ''''\n",
    "    Input: word indexing from the tocnizer appove and the pre-trined word vector model\n",
    "    \n",
    "    output: embedding matrix\n",
    "    \n",
    "    ''''\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1,300 ))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_matrix[i] = embeddings_index[\"unknown\"]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building matrices\n",
    "embedding_matrix_1 = build_matrix(word_index, crawl_embeddings)\n",
    "embedding_matrix_2 = build_matrix(word_index, glove_embeddings)\n",
    "\n",
    "# Concatenating embedding matrices \n",
    "embedding_matrix = np.concatenate([embedding_matrix_1, embedding_matrix_2], axis=1)\n",
    "\n",
    "# deleting to save spase in RAM \n",
    "del embedding_matrix_1, embedding_matrix_2\n",
    "del crawl_embeddings ,glove_embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Building Annotation based Bi-LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/Model_simple_expl.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, our input will be a sequence of word representation it will be processed in the embedded layer the output will be transferred to our bi-directional LSTM which it represents 2 LSTM layers one proceed forward and the second performs backward after this combine the result plus implement the attention method with average weights in order to get an accurate result next we do a dropout with 0.5 in order to avoid overfitting following this move to dense layer with a sigmoid activation function.\n",
    "\n",
    "You will see below the code representation of what has been discussed :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Defie the input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define shape of the input \n",
    "inp = Input(shape=(maxlen,)) #define shape of the input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 first layer (embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer is used to create word vectors for incoming words,The output of the Embedding layer is the input to the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer \n",
    "embedding_layer = Embedding(*embedding_matrix.shape,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False) # create embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass input into the embded layer \n",
    "x = embedding_layer(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Bi-LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed into bidirectional wech it will out but \n",
    "x = Bidirectional(LSTM(256, return_sequences=True))(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed into bidirectional wech it will out but\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Concatnationg Avreging and Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras attention layer\n",
    "    Reference: https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "    \"\"\"\n",
    "    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, \n",
    "                 W_constraint=None, b_constraint=None, bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = None\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        self.param_W = {\n",
    "            'initializer': initializers.get('glorot_uniform'),\n",
    "            'name': '{}_W'.format(self.name),\n",
    "            'regularizer': regularizers.get(W_regularizer),\n",
    "            'constraint': constraints.get(W_constraint)\n",
    "        }\n",
    "        self.W = None\n",
    "\n",
    "        self.param_b = {\n",
    "            'initializer': 'zero',\n",
    "            'name': '{}_b'.format(self.name),\n",
    "            'regularizer': regularizers.get(b_regularizer),\n",
    "            'constraint': constraints.get(b_constraint)\n",
    "        }\n",
    "        self.b = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.features_dim = input_shape[-1]\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],), \n",
    "                                 **self.param_W)\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],), \n",
    "                                     **self.param_b)\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        step_dim = self.step_dim\n",
    "        features_dim = self.features_dim\n",
    "\n",
    "        eij = K.reshape(\n",
    "            K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))),\n",
    "            (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the GlobalAveragePooling1D \n",
    "avrege = GlobalAveragePooling1D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the Attention \n",
    "attention = Attention(maxlen)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate these techniqes to form layer that perform on the output from the Bi-LSTM \n",
    "hidden = concatenate([attention,avrege])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dense with 512 output with relu acttivation function\n",
    "x = Dense(512, activation='relu')(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.4 Dropu out to avoid over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a dropout with 0.5 to avoid ofer fitting \n",
    "x =  Dropout(0.5)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.5 Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation: max(x, 0), the element-wise maximum of 0 and the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dense with 128 output with relu acttivation function \n",
    "x = Dense(128, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid activation function, sigmoid(x) = 1 / (1 + exp(-x))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dense output with sigmoid acttivation function \n",
    "o = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.6 Buldin the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model \n",
    "model = Model(inputs=inp, outputs=o)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.7.1  Fitting the train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting on train data set\n",
    "model.fit(X_t,y_train,batch_size=32,epochs=2,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3.7.2  Fitting the Validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fitting on Validation data set\n",
    "model.fit(X_valid,y_valid,batch_size=32,epochs=2,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.7 Prediction & Save the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predect the toxicity of the test\n",
    "val = model.predict(X_te, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predections into the submetion file \n",
    "sub['toxic'] = val \n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4  Model AUC = 74.83\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
    "- https://www.researchgate.net/publication/321976542_Bidirectional_LSTM_Recurrent_Neural_Network_for_Keyphrase_Extraction\n",
    "- https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e\n",
    "- https://www.researchgate.net/publication/323130660_Text_Classification_Research_with_Attention-based_Recurrent_Neural_Networks\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- https://www.sciencedirect.com/science/article/abs/pii/S0925231219301067\n",
    "- https://keras.io\n",
    "- https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6834909.pdf\n",
    "- https://towardsdatascience.com/natural-language-processing-from-basics-to-using-rnn-and-lstm-ef6779e4ae66\n",
    "- https://colah.github.io/posts/2015-08-Understanding-LSTMs\n",
    "- https://towardsdatascience.com/understanding-rnn-and-lstm-f7cdf6dfc14e\n",
    "- https://www.sciencedirect.com/science/article/abs/pii/S0925231219301067\n",
    "- https://www.kaggle.com/authman/pickled-crawl300d2m-for-kernel-competitions\n",
    "- https://www.kaggle.com/authman/pickled-glove840b300d-for-10sec-loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
